{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer of Transition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', \n",
    "                       ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "    \n",
    "    def push(self, *args):\n",
    "        if len(self.memory) == self.capacity:\n",
    "            self.memory.pop()\n",
    "        \n",
    "        self.memory.insert(0, Transition(*arg))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size) # a list of element from self.memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic - Q Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        '''\n",
    "        2 Hidden layers\n",
    "        400 & 300 units each\n",
    "        action is only introduced in 2nd hidden layer\n",
    "        '''\n",
    "        super(Critic, self).__init__()\n",
    "        self.l1 = nn.Linear(state_dim, 400)\n",
    "        self.l2 = nn.Linear(400 + action_dim, 300)\n",
    "        self.l3 = nn.Linear(300, 1)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = F.relu(self.l1(state))\n",
    "        x = F.relu(self.l2(torch.cat([x, action], dim=1)))\n",
    "        x = self.l3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor - Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, sup_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.l1 = nn.Linear(state_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, action_dim)\n",
    "        self.sup_action = sup_action  # suppermum of action space\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.l1(state))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.sup_action * torch.tanh(self.l3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG (mainly everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(object):\n",
    "    def __init__(self, state_dim, action_dim, sup_action, memory_capacity, batch_size, \n",
    "                 gamma=0.99, actor_lr=1e-3, polyak=0.99):\n",
    "        # critic\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_optimizer = optim.RMSprop(self.critic.parameters())\n",
    "        \n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_target.eval()\n",
    "        # actor\n",
    "        self.actor = Actor(state_dim, action_dim, sup_action).to(device)\n",
    "        self.actor_optimizer = optim.RMSprop(self.actor.parameters())\n",
    "        \n",
    "        self.actor_target = Actor(state_dim, action_dim, sup_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_target.eval()\n",
    "        # replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(capacity)\n",
    "        self.states_batch = None  # store the state batch sampled from replay buffer for later use in update_actor\n",
    "        # hyper parameters\n",
    "        self.BATCH_SIZE = batch_size\n",
    "        self.GAMMA = gamma\n",
    "        self.actor_lr = actor_lr\n",
    "        self.polyak = polyak\n",
    "    \n",
    "    def update_critic(self):\n",
    "        # sample exp from replay buffer\n",
    "        transitions = self.replay_buffer.sample(self.BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        # pull out index (i.e. mask) in the batch & next_state of transitions having next_state is not None\n",
    "        mask_nonFinal_next_states = torch.tensor(tuple(map(lambda s: s is not None, \n",
    "                                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "        nonFinal_next_states = torch.cat([s for s in batch.next_state\n",
    "                                             if s is not None])\n",
    "        # construct tensor of state, action, reward\n",
    "        self.states_batch = torch.cat(batch.state)\n",
    "        actions_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        # compute TD target\n",
    "        next_state_act_val = torch.zeros(self.BATCH_SIZE, device=device)\n",
    "        next_state_act_val[mask_nonFinal_next_states] = self.critic_target(nonFinal_next_states, \n",
    "                                                                        self.actor_target(nonFinal_next_states))\n",
    "        td_target = reward_batch + self.GAMMA * next_state_act_val\n",
    "        \n",
    "        loss = F.mse_loss(self.critic(self.states_batch, actions_batch), td_target)  # MS of td_error\n",
    "        \n",
    "        # call optimizer of critic to minimize loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "    \n",
    "    def update_actor(self):\n",
    "        loss = -self.critic(self.states_batch, actor(self.states_batch)).mean() \n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "    def update_target_net(self):\n",
    "        with torch.no_grad():\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_((1 - self.polyak) * param.data + self.polyak * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_((1 - self.polyak) * param.data + self.polyak * target_param.data)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "    \n",
    "    def train(self, _env):\n",
    "        #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"LunarLanderContinuous-v2\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "sup_action = env.action_space.high\n",
    "algo = DDPG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0, 1, 2]\n",
    "a.pop()\n",
    "print(a)\n",
    "a.insert(0, -12)\n",
    "a\n",
    "random.sample(a, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
