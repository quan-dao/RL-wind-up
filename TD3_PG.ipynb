{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mquan/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "from math import sqrt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(\"LunarLanderContinuous-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dim, 400)\n",
    "        self.layer2 = nn.Linear(400, 300)\n",
    "        self.layer3 = nn.Linear(300, action_dim)\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        \n",
    "        # weight init\n",
    "        nn.init.uniform_(self.layer1.weight, -1/sqrt(state_dim), 1/sqrt(state_dim))\n",
    "        nn.init.uniform_(self.layer2.weight, -1./sqrt(400), 1./sqrt(400))\n",
    "        nn.init.uniform_(self.layer3.weight, -3*1e-3, 3*1e-3)\n",
    "        nn.init.uniform_(self.layer1.bias, -1/sqrt(state_dim), 1/sqrt(state_dim))\n",
    "        nn.init.uniform_(self.layer2.bias, -1./sqrt(400), 1./sqrt(400))\n",
    "        nn.init.uniform_(self.layer3.bias, -3*1e-3, 3*1e-3)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.layer1(state))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = torch.tanh(self.layer3(x)) * self.max_action\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic\n",
    "A feedforward network with 2 hiddens layer. \n",
    "* 1st layer: 400 units, 2nd layer: 300 units\n",
    "* Activation: first 2 layer ReLU, last layer: None\n",
    "\n",
    "Note: Action is included at the input to the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer2 = nn.Linear(400, 300)\n",
    "        self.layer3 = nn.Linear(300, 1)\n",
    "        \n",
    "        # weight init\n",
    "        nn.init.uniform_(self.layer1.weight, -1/sqrt(state_dim + action_dim), 1/sqrt(state_dim + action_dim))\n",
    "        nn.init.uniform_(self.layer2.weight, -1./sqrt(400), 1./sqrt(400))\n",
    "        nn.init.uniform_(self.layer3.weight, -3*1e-3, 3*1e-3)\n",
    "        nn.init.uniform_(self.layer1.bias, -1/sqrt(state_dim + action_dim), 1/sqrt(state_dim + action_dim))\n",
    "        nn.init.uniform_(self.layer2.bias, -1./sqrt(400), 1./sqrt(400))\n",
    "        nn.init.uniform_(self.layer3.bias, -3*1e-3, 3*1e-3)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = F.relu(self.layer1(torch.cat([state, action], dim=1)))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer\n",
    "Replay Buffer is a list of Transitions. Transitions are named tuples having 4 fields:\n",
    "* state\n",
    "* action\n",
    "* reward\n",
    "* next state\n",
    "* done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', \n",
    "                        ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "    \n",
    "    def push(self, *args):\n",
    "        if self.is_full():\n",
    "            self.memory.pop()\n",
    "        self.memory.insert(0, Transition(*args))\n",
    "    \n",
    "    def is_full(self):\n",
    "        return len(self.memory) == self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        transit = random.sample(self.memory, batch_size)\n",
    "        return Transition(*zip(*transit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD3 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(object):\n",
    "    def __init__(self, state_dim, action_dim, max_action, min_action, capacity,\n",
    "                 learning_rate=1e-3, batch_size=100, tau=0.005, gamma=0.99):\n",
    "        self.max_action = max_action\n",
    "        self.min_action = min_action\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        # init critic\n",
    "        self.critic1 = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic2 = Critic(state_dim, action_dim).to(device)\n",
    "        \n",
    "        self.critic1_optim = optim.Adam(self.critic1.parameters(), lr=learning_rate, weight_decay=1e-2)\n",
    "        self.critic2_optim = optim.Adam(self.critic2.parameters(), lr=learning_rate, weight_decay=1e-2)\n",
    "        \n",
    "        self.critic1_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic1_target.load_state_dict(self.critic1.state_dict())\n",
    "        \n",
    "        self.critic2_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic2_target.load_state_dict(self.critic2.state_dict())\n",
    "        \n",
    "        # init actor\n",
    "        self.actor = Actor(state_dim, action_dim, max_action)\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(capacity)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def train(self, update_policy, iteration=1):\n",
    "        for it in range(iteration):\n",
    "            # sampe batch of transitions from replay memory\n",
    "            batch = self.replay_buffer.sample(self.batch_size)\n",
    "            state = torch.cat(batch.state)\n",
    "            action = torch.cat(batch.action)\n",
    "            reward = torch.cat(batch.reward)\n",
    "            next_state = torch.cat(batch.next_state)\n",
    "            d = torch.cat(batch.done)\n",
    "            \n",
    "            # compute target action\n",
    "            target_action = self.actor_target(next_state)\n",
    "            epsilon = np.random.normal(0, 0.2, size=list(target_action.shape)).clip(-0.5, 0.5) # create noise\n",
    "            target_action += torch.tensor(epsilon, device=device, dtype=torch.float)\n",
    "            target_action = torch.max(torch.min(target_action, self.max_action), self.min_action)  # clipped\n",
    "            \n",
    "            # compute Q_target\n",
    "            Q_target1 = self.critic1_target(next_state, target_action)\n",
    "            Q_target2 = self.critic2_target(next_state, target_action)\n",
    "            td_target = reward + self.gamma * (1 - d) * torch.min(Q_target1, Q_target2).detach()\n",
    "            \n",
    "            # update critic \n",
    "            critic1_loss = F.mse_loss(self.critic1(state, action), td_target)\n",
    "            self.critic1_optim.zero_grad()\n",
    "            critic1_loss.backward()\n",
    "            self.critic1_optim.step()\n",
    "            \n",
    "            critic2_loss = F.mse_loss(self.critic2(state, action), td_target)\n",
    "            self.critic2_optim.zero_grad()\n",
    "            critic2_loss.backward()\n",
    "            self.critic2_optim.step()\n",
    "            \n",
    "            if update_policy:\n",
    "                actor_loss = -self.critic1(state, self.actor(state)).mean()\n",
    "                self.actor_optim.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optim.step()\n",
    "                \n",
    "                # Update target network\n",
    "                for param, target_param in zip(self.critic1.parameters(), self.critic1_target.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "                    \n",
    "                for param, target_param in zip(self.critic2.parameters(), self.critic2_target.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "                    \n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    def select_action(self, state, random_action=None):\n",
    "        assert torch.is_tensor(state)\n",
    "        if random_action == None:\n",
    "            # passed the initial acting with random policy phase\n",
    "            action = self.actor(state)\n",
    "            epsilon = np.random.normal(0, 0.1, size=list(action.shape))\n",
    "            action += torch.tensor(epsilon, device=device, dtype=torch.float)  # add noise\n",
    "            action = torch.max(torch.min(action, self.max_action), self.min_action)  # clipped\n",
    "        else:\n",
    "            action = torch.tensor(random_action, device=device, dtype=torch.float)\n",
    "        return action\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "max_action = torch.tensor(env.action_space.high, device=device, dtype=torch.float)\n",
    "min_action = torch.tensor(env.action_space.low, device=device, dtype=torch.float)\n",
    "\n",
    "capacity = int(1e6)\n",
    "\n",
    "agent = TD3(state_dim, action_dim, max_action, min_action, capacity)\n",
    "\n",
    "max_random_action_steps = 3000\n",
    "random_action_steps = 0\n",
    "flag_collecting_exp = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(np_array):\n",
    "    '''\n",
    "    convert 1d np array to tensor (written as a row vector)\n",
    "    '''\n",
    "    return torch.FloatTensor(np_array.reshape(1, -1)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episode = 1\n",
    "for i in range(max_episode):\n",
    "    obs = env.reset()\n",
    "    state = to_tensor(obs)\n",
    "    for step in count():\n",
    "        # choose action\n",
    "        if flag_collecting_exp:\n",
    "            action = agent.select_action(state, env.action_space.sample())\n",
    "            random_action_steps += 1\n",
    "        else:\n",
    "            action = agent.select_action(state)\n",
    "        \n",
    "        # observe reward & next state\n",
    "        next_obs, reward, done, _ = env.step(action.cpu().data.numpy().flatten())\n",
    "        \n",
    "        next_state = to_tensor(next_obs)\n",
    "        reward = to_tensor(np.array([[reward]]))\n",
    "        # store transition in replay buffer\n",
    "        agent.replay_buffer.push(state, action, reward, next_state, float(done))\n",
    "        \n",
    "        # check if initial random policy phase has passed\n",
    "        if random_action_steps > max_random_action_steps:\n",
    "            flag_collecting_exp = False  # stop random policy\n",
    "        \n",
    "        if not flag_collecting_exp:\n",
    "            if step % 2 == 0:\n",
    "                update_policy = True\n",
    "            else:\n",
    "                update_policy = False\n",
    "            # call the training method\n",
    "            agent.train(update_policy)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        # Move on\n",
    "        state.data.copy_(next_state.data)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING AREA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test replay buffer\n",
    "rp = ReplayBuffer(10)\n",
    "s = env.reset()\n",
    "for i in range(5):\n",
    "    a = env.action_space.sample()\n",
    "    n_s, reward, done, _ = env.step(a)\n",
    "    rp.push(s, a, reward, n_s)\n",
    "    s = n_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.5744464843144739, -0.9941939859433546)\n",
      "(array([0.9572367 , 0.59831715], dtype=float32), array([-0.76345116,  0.27984205], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "batch = rp.sample(2)\n",
    "print(batch.reward)\n",
    "print(batch.action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(np.array([[0, 5, 4]]).T)\n",
    "b = torch.tensor(np.array([[1, 6, 2]]).T)\n",
    "# min_ab, _ = torch.min(torch.cat([a, b], dim=1), dim=1, keepdim=True)\n",
    "min_ab = torch.min(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [5],\n",
      "        [2]]) \t torch.Size([3, 1])\n",
      "tensor([[  1],\n",
      "        [-24],\n",
      "        [ -6]])\n"
     ]
    }
   ],
   "source": [
    "print(min_ab,\"\\t\", min_ab.shape)\n",
    "print((1 - a) * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of backend CUDA but got backend CPU for argument #2 'other'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-a964e0318030>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mhigh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mclipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of backend CUDA but got backend CPU for argument #2 'other'"
     ]
    }
   ],
   "source": [
    "N = 2\n",
    "act = np.concatenate([env.action_space.sample().reshape(1,-1) for i in range(N)], axis=0)\n",
    "act = torch.tensor(act, dtype=torch.float).to(device)\n",
    "epsilon = np.random.normal(0, 0.2, size=list(act.shape)).clip(-0.5, 0.5)\n",
    "\n",
    "act += torch.tensor(epsilon, device=device, dtype=torch.float)\n",
    "\n",
    "low = torch.tensor(env.action_space.low)\n",
    "high = torch.tensor(env.action_space.high)\n",
    "\n",
    "clipped = torch.max(torch.min(act, high), low)\n",
    "print(act)\n",
    "print(\"----\")\n",
    "print(clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1.], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(list(act.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 130 timesteps\n",
      "observation:\n",
      " [ 0.48910475 -0.22189607  1.5950245  -0.860313   -0.7419953   3.0693738\n",
      "  1.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    for t in range(1000):\n",
    "#         env.render()\n",
    "#         print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            print(\"observation:\\n\", observation)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
